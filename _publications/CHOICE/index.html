<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CHOICE: Coordinated Human-Object Interaction in Cluttered Environments for Pick-and-Place Actions</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CHOICE: Coordinated Human-Object Interaction in Cluttered Environments for Pick-and-Place Actions</h1>
                <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                  <!-- <span class="author-block">
                    <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,
                  </span>
                  <span class="author-block">
                    <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,
                  </span>
                  <span class="author-block">
                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span> -->
                 
                  <span class="author-block">
                      Jintao Lu<sup>1</sup>,
                  </span>
                  <span class="author-block">
                      He Zhang<sup>2</sup>,
                  </span>
                  <span class="author-block">
                      Yuting Ye<sup>3</sup>,
                  </span>
                  <span class="author-block">
                      Takaaki Shiratori<sup>3</sup>,
                  </span>
                  <span class="author-block">
                      Sebastian Starke<sup>3</sup>,
                  </span>
                  <span class="author-block">
                      Taku Komura<sup>1</sup>,
                  </span>
                </div>
                  
               
                  <div class="is-size-4 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Hong Kong  </span>
                    <span class="author-block"><sup>2</sup>Tencent Robotics X</span>
                    <span class="author-block"><sup>3</sup>Meta Reality Lab  </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.06702" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://connecthkuhk-my.sharepoint.com/:u:/g/personal/u3008430_connect_hku_hk/EYdBUoqeejVCp8SgqodZHR0Bgb6LiMIAM5dCRdO2oAF68w?e=7kjyTl" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span> 

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/lujintaozju" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming at 2026.1)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                 <span class="link-block">
                  <a href="http://arxiv.org/abs/2412.06702" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> 

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe width="560" height="315" src="https://www.youtube.com/embed/TzzPvkqb72A?si=8k4TmGotj7wB4_V_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
            <!--iframe src="https://youtu.be/TzzPvkqb72A?si=CLftJiY8utUXdGnD" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe-->
        </div>
      
    </div>
  </div>
</section>
<!-- End teaser video -->




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Animating human-scene interactions such as pick-and-place tasks in cluttered, complex layouts is a challenging task, with objects of a wide variation of geometries and articulation under scenarios with various obstacles. The main difficulty lies in the sparsity of the motion data compared to the wide variation of the objects and environments as well as the poor availability of transition motions between different tasks, increasing the complexity of the generalization to arbitrary conditions. To cope with this issue, we develop a system that tackles the interaction synthesis problem as a hierarchical goal-driven task. Firstly, we develop a bimanual scheduler that plans a set of keyframes for simultaneously controlling the two hands to efficiently achieve the pick-and-place task from an abstract goal signal such as the target object selected by the user. Next, we develop a neural implicit planner that generates guidance hand trajectories under diverse object shape/types and obstacle layouts. Finally, we propose a linear dynamic model for our DeepPhase controller that incorporates a Kalman filter to enable smooth transitions in the frequency domain, resulting in a more realistic and effective multi-objective control of the this system can produce a wide range of natural pick-and-place movements with respect to the geometry of objects, the articulation of containers and the layout of the objects in the scene.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- End video carousel -->





<!-- Paper poster -->
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Overview</h2>
        <div class="content has-text-justified">
          <div class="col-12">
            <img src="static/images/pipeline.png"/>
          </div>
          <p>We split the interaction synthesis task into three major sub-tasks:</p>
          <ul>
            <li>
              <span class="highlight"><b>Hand-object trajectory planning</b></span>, managed by the trajectory planning sub-system (brown) shown in 
              <a href="#pipeline">Fig. 2</a> and 
              <a href="#section-INTP">Sec. 4</a>.
            </li>
            <li>
              <span class="highlight"><b>Bimanual interaction scheduling</b></span>, managed by the goal matching sub-system (grey) and the goal coordination (cyan) 
              shown in <a href="#pipeline">Fig. 2</a> and 
              <a href="#section-bimanual">Sec. 5</a>.
            </li>
            <li>
              <span class="highlight"><b>Fully-body motion control</b></span>, managed by the goal-driven control sub-system (blue) shown in
              <a href="#pipeline">Fig. 2</a> and 
              <a href="#section-InteractionController">Sec.6</a>.
            </li>
          </ul>
          <p>
            When the user clicks the object to pick or place, the path to walk towards the object is first planned, 
            then the series of keyframes for the key joints (left/right wrist and hip) based on contacts are scheduled 
            and finally, collision-free trajectories of the arms to conduct the motion is computed. The DeepPhase-based controller then synthesizes the motion to follow the scheduled sub-goals and planned trajectories in real-time in the phase manifold.
          </p>

            <br><br>
            

            <!-- this is for colum wise display -->
            <div class="container">
              <div class="row">
                
              <!--   <div class="column is-5">
                    <img src="static/images/creep.gif"/>
                    <h3 class="center">Sub1 Text</h3>
                </div>
                <div class="column is-5">
                    <img src="static/images/creep.gif"/>
                    <h3 class="center">Sub2 Text</h3>
                </div>
                <br><br>
                <h3 class="center">Main Text </h3> -->
                <div class="col-12">
                  <h2 class="center m-bottom">Long-term Interaction Performing under Diverse Interaction Targets</h2>
                </div>

                <br><br><br>

                <div class="col-6">
                    <video controls width="100%">
                        <source src="static/videos/shortDemo.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="col-6">
                    <video controls width="100%">
                        <source src="static/videos/longDemo.mp4" type="video/mp4">
                    </video>
                </div>
              
                <div class="col-12">
                  <h2 class="center m-bottom">Bimanual Operations Adaptive to Diverse Layouts</h2>
                </div>

                <br><br><br>

                <div class="col-6">
                    <video controls width="100%">
                        <source src="static/videos/diverseUpper.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="col-6">
                    <video controls width="100%">
                        <source src="static/videos/StateMachineAll.mp4" type="video/mp4">
                    </video>
                </div>

              </div>
            </div>

            <br><br>
          
            




        </p>
      </div>
      <h2 class="title">Dataset</h2>
        <div class="content has-text-justified">
          <div class="col-12">
            <img src="static/images/dataPipeline.png"/>
          </div>
          <div class="col-12">
              <video controls width="100%">
                  <source src="static/videos/CookingData.mp4" type="video/mp4">
              </video>
          </div>
          <p>Dataset can be downloaded from: <a href="https://connecthkuhk-my.sharepoint.com/:u:/g/personal/u3008430_connect_hku_hk/EYdBUoqeejVCp8SgqodZHR0Bgb6LiMIAM5dCRdO2oAF68w?e=7kjyTl">here</a>, just following the description in Readme.md there to visualize/export the data.
          </p>
          <p>We extensively captured and post-processed 150 motion sequences using the Vicon Sh√≥gun 1 motion capture system. The dataset comprises long-term, delicate interactions in lifelike scenes, such as a corridor with a drinking bar, and a well-equipped kitchen room. 
          </p>

    </div>
  </div>
</section>












<!--End paper poster -->










<!--BibTex citation -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-12">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
